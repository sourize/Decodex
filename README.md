# Decodex: A GPT Decoder-Only Model from Scratch

A minimal yet complete implementation of a decoder-only GPT model built entirely from scratch using PyTorch. This project demonstrates the core concepts of the Transformer architecture through character-level text generation.

## ğŸš€ Features

- **Pure PyTorch implementation** - No pre-built transformer modules, every component built from scratch
- **Character-level tokenization** - Simple yet effective approach for text processing
- **Multi-head self-attention** - Scalable attention mechanism with configurable heads
- **Positional embeddings** - Maintains positional information in sequences
- **Regularization techniques** - Layer normalization and dropout for training stability
- **Autoregressive generation** - Iterative text generation with configurable length

## ğŸ—ï¸ Architecture

The model implements a classic decoder-only Transformer design:

```
Input Text â†’ Token Embeddings + Positional Embeddings
           â†“
    Transformer Blocks (Ã—4)
    â”œâ”€â”€ Multi-Head Self-Attention
    â”œâ”€â”€ Layer Normalization
    â”œâ”€â”€ Feed-Forward Network
    â””â”€â”€ Residual Connections
           â†“
    Linear Projection â†’ Output Probabilities
```

Each transformer block contains:
- Multi-head self-attention mechanism
- Position-wise feed-forward network
- Layer normalization and residual connections

## âš™ï¸ Configuration

| Hyperparameter | Value | Description |
|----------------|-------|-------------|
| Batch Size | 16 | Training batch size |
| Block Size | 32 | Maximum sequence length |
| Embedding Dimension | 64 | Token embedding size |
| Attention Heads | 4 | Number of attention heads |
| Transformer Layers | 4 | Number of transformer blocks |
| Learning Rate | 1e-3 | Adam optimizer learning rate |
| Training Steps | 5000 | Maximum training iterations |
| Dropout Rate | 0.0 | Dropout probability |

## ğŸ“š Training Data

The model trains on classic literature for character-level text generation:
- **The Adventure of the Empty House** (Sherlock Holmes)
- **Tiny Shakespeare** dataset

Text is tokenized at the character level, creating a vocabulary from unique characters in the training corpus.

## ğŸ› ï¸ Installation

**Prerequisites:**
- Python 3.7+
- PyTorch

**Setup:**
```bash
# Install PyTorch
pip install torch

# Clone the repository
git clone https://github.com/sourize/Decodex.git
cd Decodex
```

## ğŸš€ Usage

### Training
Start training the model from scratch:
```bash
python decodex.py
```

### Text Generation
Generate text after training:
```python
import torch

# Initialize generation context
context = torch.zeros((1, 1), dtype=torch.long, device=device)

# Generate text
generated_text = model.generate(context, max_new_tokens=2000)
output = decode(generated_text[0].tolist())
print(output)
```

## ğŸ“Š Sample Output

The trained model produces text in the style of its training data:

```
Sherlock Holmes stood at the door, his keen eyes scanning the room.
"Watson," he said, "observe the details. The game is afoot!"
```

## ğŸ”® Future Enhancements

- [ ] **Scaled training** - Train on larger, diverse datasets
- [ ] **Advanced tokenization** - Implement BPE or SentencePiece
- [ ] **Optimization improvements** - Add learning rate scheduling and gradient clipping
- [ ] **Model scaling** - Experiment with larger architectures
- [ ] **Evaluation metrics** - Add perplexity and other generation quality metrics
- [ ] **Interactive demo** - Web interface for text generation

## ğŸ“– References

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - The original Transformer paper
- [Andrej Karpathy's "Let's Build GPT"](https://www.youtube.com/watch?v=kCc8FmEb1nY) - Educational implementation guide

## ğŸ“„ License

MIT License - feel free to use this code for learning and experimentation.

---

â­ **Star this repo** if you found it helpful for understanding Transformers from scratch!
